import { FaceDetection, TNetInput } from "@vladmandic/face-api";
import { Stream } from "../../helpers/Stream";
export declare class Detector {
  static instance: Detector;
  private static tinyFaceDetectorOptions;
  private static readonly BACKEND;
  static debug: boolean;
  private videoElement;
  private readonly isMobile;
  private readonly modelPath;
  private requestAnimationFrame;
  private probabilityThreshold;
  private counterSuccessfulResults;
  private MAX_FPS_DETECTION;
  private rendering;
  private readonly MAX_NUMBER_FACES;
  private readonly MAX_ANGLE_TURN_HEAD;
  private readonly MIN_FACE_SIZE_FOR_MOBILE;
  private readonly MIN_FACE_SIZE_FOR_DESKTOP;
  private readonly MIN_OCCUPIED_SPACE_FOR_DESKTOP;
  private readonly MIN_OCCUPIED_SPACE_FOR_MOBILE;
  private readonly X_OFFSET_FROM_FRAME;
  private readonly Y_OFFSET_FROM_FRAME;
  private DEFAULT_NUMBER_SUCCESSFUL_RESULTS_FOR_AUTO_CAPTURING;
  private videoRatio;
  private showDetectedFace;
  private headTurnCounter;
  private counterSuccessfulHeadTurns;
  private innerCanvas;
  private stream;
  private static isIos;
  setProbabilityThreshold(val: number): void;
  private constructor();
  static getInstance(stream?: Stream, isMobile?: boolean, modelPath?: string): Detector;
  updateHtmlElements(videoElement: HTMLVideoElement): void;
  static initDetector(modelPath: string): Promise<void>;
  static loadingModels(modelPath: string): Promise<void>;
  static modelsLoaded(): boolean;
  static warmingUpModels(): Promise<void>;
  static detectAllFaces(src: TNetInput): Promise<FaceDetection[]>;
  startDetector(): Promise<void>;
  stopDetector(): void;
  private updateForIos;
  private detection;
  private checkDetectedFaceOnError;
  private autoCapturing;
  private checkNumberFaces;
  private checkProbability;
  private checkFaceSize;
  private checkFaceIndent;
  private checkHeadRotation;
  private drawFaces;
  createCanvasForDrawingMask(video: HTMLVideoElement): void;
}
